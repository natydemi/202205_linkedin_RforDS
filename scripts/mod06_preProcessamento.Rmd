---
title: "Arrumação e Transformação de Dados"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE)
```

# Bibliotecas

```{r}
library(tidyverse)
```


# Introdução
Neste arquivo ...


# Pré-processamento

A etapa de pré-processamento faz uso de todos os recursos que utilizamos até aqui. Tendo como enfoque a qualidade do dado, algumas das ações que podemos relacionar a esta etapa, organizadas por grupos, são:

- Importação
  - Identificar unidade de análise 
  - Agregar dados segundo a unidade identificada
  - Tratar registros inválidos e linhas duplicadas
  - Tornar únicos os nomes das colunas
  - Combinar bases de dados


- Perfil 
  - Gerar metadados/metainformação dos dados
  - Verificar tipos de dados e unidades de medida
  - Corrigir formatações incorretas
  - Identificar amostras tendenciosas 
  
  
- Limpeza
  - Remover colunas irrelevantes
  - Filtrar observações fora do escopo de interesse


- Ajustes pré modelagem (**dataprep**)
  - Tratar dados faltantes (missing data)
  - Aplicar codificações
  - Aplicar padronizações


Note que se tratam de ajustes que podem fazer a diferença entre o sucesso e o fracasso da sua análise, e não vou te enganar, isso dá trabalho! Algumas estimativas indicam que até 80% do tempo de uma análise de dados pode se concentrar aqui. Desde abreviações e espaços extras indevidos, até dados faltantes que podem indicar a necessidade de redirecionamento da sua análise, é na etapa de pré-processamento que garantimos que os dados estão aptos a representarem a realidade que esperamos que seja retratada.

- falar de outliers: Valores extremos, também conhecidos como outliers, ou valores aberrantes, são características facilmente encontradas em bases de dados reais, e com alto potencial de influência nas análises. Neste video iremos discutir sobre a importancia de identificar tais características, e formas de endereça-las.

   
## E no R? 

Aqui o desafio é mais investigativo e julgamental do que em termos de ferramenta. Com o que vimos nos módulos anteriores, você será capaz de avaliar as questões e endereçar soluções. Com certeza existem pacotes que se mostram mais práticos para determinados estudos, como por exemplo `codemeta` que nos auxiliar com a criação de metadados, ou o pacote `naniar` que nos auxilia com dados faltantes (falaremos dele logo mais). Mas é importante ressaltar que, em última instância, é uma questão de pesquisar `library R missing data`, por exemplo — DICA: pesquisas em inglês tendem a ser mais efetivas ;). O verdadeiro conhecimento nesta etapa é ter um horizonte sobre O QUE precisa ser feito, mais do que o COMO. 


## Ajustes pré modelagem (dataprep)

Apesar dos comentários acima, existe um grupo em particular que comentaremos um pouco mais. No caso, o grupo que denominamos `Ajustes pré modelagem`, ou ainda dataprep. Tais pré-processamentos são particularmente importantes pois, a depender da técnica de modelagem que você deseje aplicar, estes ajustes podem ser obrigatórios — dado que algumas técnicas não aceitam valores faltantes ou features categóricas como entrada. Sendo assim, teremos uma breve discussão sobre o que envolve cada um dos `dataprep` citados. No caso dos dados faltantes, visto a sua relevância, daremos uma profundidade maior à discussão.


### Aplicar codificações

De forma geral as técnicas de modelagem utilizadas no contexto de análise de dados são capazes de lidar apenas com valores numéricos. Mas, se temos em uma base de dados variáveis como nível de escolaridade, em que os  valores podem ser "fundamental", "médio" ou "superior" ou estado de nascimento, nos quais podemos ter 27 categorias, não é razoável que a gente simplesmente deixe de usar essas informações, pois podem ser de grande importância na explicação do problema. Assim, para que esses dados não sejam disperdiçados utilizamos técnicas de Codificação de Variáveis, que nada mais é do que o processo de transformar variáveis categóricas em  númericas, logo, adequadas às técnicas de modelagem.

Existem várias estratégias para fazer isso e, em geral, o desafio é saber qual é a mais adequada para cada contexto. Algumas estratégias comuns são:

- Variáveis Dummy (`recipes::step_dummy()`) 
- One Hot Encoding (`recipes::step_dummy(.x, one_hot = T)`): cria-se uma nova variável para cada categoria da variável original
- Tokenize (`recipes::step_tokenize()`): 
- Hash  (`textrecipes::step_dummy_hash()`): convert nominal data (e.g. character or factors) into one or more numeric binary columns using the levels of the original data.
- Label Encoding: cada categoria passa a ser representada por um número
- Frequency Encoding: O valor numérico atribuído a cada categoria é a quantidade de vezes em que ela aparece na base

Aqui é importante ter em conta pontos como: quantidade de categorias existentes, nosso interesse em manter a interpretabilidade dos resultados, ou ainda pontos relacionados a própria modelagem (como questões de multicolineariedade). 


### Aplicar padronizações

Conforme comentado, as técnicas de modelagem lidam, em geral, com valores numéricos e não sabem exatamente o que eles representam. Quando temos variáveis com magnitudes e variações muito diferentes como, por exemplo, idade e salário, o modelo pode assumir que a variável de maior magnitude é mais relevante, o que muitas vezes não é verdade, e dar um peso maior para ela. Por isso utilizamos técnicas de padronização, que são métodos para fazer com que as variáveis tenham uma escala parecida e, muitas vezes, fixa. Com isso garantimos que cada variável tenha uma contribuição parecida no modelo e, em alguns casos, essa técnica também torna o processo de modelagem mais rápido.

Vamos repassar um exemplo:

```{r}
dados::casas %>% 
  select(where(is.numeric)) %>% 
  glimpse()
```

```{r}
dados_porao <- dados::casas %>% 
  select(lote_area, venda_valor, acima_solo_num_comodos, alvenaria_area) 


p1 <- dados_porao %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(value, name, color = name)) +
  geom_boxplot() +
  theme_minimal() +
  theme(legend.position = "bottom", legend.title=element_blank()) +
  labs(x = "", y = "") 


p2 <- dados_porao %>% 
  scale() %>% 
  as_tibble() %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(name, value, color = name)) +
  geom_boxplot() +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(x = "", y = "") +
  ggtitle("Standardization Z-Score") +
  guides(x = "none")


p3 <- dados_porao %>% 
  mutate(across(everything(), ~scales::rescale(.x, to = c(0, 1)))) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(name, value, color = name)) +
  geom_boxplot() +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(x = "", y = "") +
  ggtitle("Min-Max Scaling") +
  guides(x = "none")
```

```{r warning=FALSE, fig.width = 7, fig.height = 5}
library(patchwork)

p1 / (p2 + p3) + 
  plot_annotation(title = 'Features Numéricas')
```

Note a similaridade desta discussão com o módulo sobre as investigações iniciais, particularmente a questão de medidas de sumarização absolutos vs. relativas ;)


### Tratar dados faltantes (missing)

Aqui, nosso objetivo primário deve ser entender qual o mecanismo que causou a ausência dos dados. Tendo como referência a discussão feita no [Capítulo 08 do livro Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/handling-missing-data.html), temos como três principais causas de ausência de dados:


- **Deficiência Estrutural** : um nível da variável que foi excluído dos dados. Por exemplo, para uma feature que caracteriza um componente que não existe na maioria das observações, poderia se usada uma categoria para indicar sua inexistência

- **Causas Específicas** : há ocorrência de evento específico que gera a falta de dados, como quando um paciente interrompe sua participação em um estudo clínico devido a um efeito adverso ou morte.

- **Ocorrências Aleatórias** : quando não é possível identificar uma causa específica para a ausência dos dados.

Em termos de endereçamento, existem diferentes modos de atuar. Podemos:

- Deleter a coluna  
- Imputar valores 
- Criar uma nova categoria

Tendo que, para cada uma dessas, é requerido algum grau de investigação e decisão, como por exemplo: qual será o valor da imputação? A média da coluna? A mediana? Utilizaremos alguma técnica baseada em modelos? Cada caso será um caso.

Vamos agora comentar um pouco sobre o ferramental relacionado aos valores faltantes:

#### no R

Uma vez que você conhece a fonte de origem dos valores faltantes, pode concluir que substitui-los é o melhor caminho. Por exemplo, você pode saber que todos os valores de "N/A", "NA" e "Não disponível" ou -99 ou -1 devem estar ausentes.



#### pacote tidyr

Comentando um pouco sobre o ferramental, o pacote `tidyr` possui algumas funções para tratar missing data, são elas:

  - `drop_na()` - excluí todas as linhas que apresentam elementos faltantes. 
  - `fill()` - substituí os dados faltantes pelo valor mais recente da referida coluna.
  - `replace_na()` - substituí os dados faltantes por um valor especificado.
  
Alguns exemplos simples
```{r}
#base para exemplo
  airquality[1:5,] 

#exemplo: exclusão das linhas identificadas com NA
  airquality[1:5,] %>% drop_na() 
  airquality[1:5,] %>% fill(c(Ozone, Solar.R)) 
  airquality[1:5,] %>% replace_na(list(Ozone=0, Solar.R=0)) 

```


Mas existem pacotes mais robustos, como é o exemplo do pacote naniar:

#### pacote naniar


```{r}
ggplot(airquality, 
       aes(x = Solar.R, 
           y = Ozone)) + 
  naniar::geom_miss_point()
```


```{r}
naniar::gg_miss_var(airquality)
```




```{r}
naniar::gg_miss_var(airquality, facet = Month)
```





# Feature Engineering

Na wikipedia, a definição de Feature Engineering, o que em português seria algo como Engenharia de Atributos é: "descoberta de variáveis, é o processo de usar o conhecimento da área de negócio, para extrair features (características, propriedades, atributos)". E é exatamente disto que se trata, de traduzir o conhecimento existente em features, de modo que o fênomeno que estamos estudando esteja melhor representado. Gosto muito de pensar na palavra eloquente aqui :) 

E como isto tende a ficar muito abstrato, organizei alguns grupos que podem nos ajudar a dar um horizonte maior do que é possível:


- **Evolução** ~ cálculo de tendência, sazonalidade e calendário
  - coeficiente angular, média móvel, dia da semana, etc.

- **Representação** ~ ajuste de unidades de medida
	- Recencia, contagem, proporção, etc

- **Relações funcionais** ~ descrição por meio de funções matemáticas
  - Transformação Box-cox, splines, etc

- **Referências** ~ informações relacionadas à área de estudo 
- Conhecimento externo


Em termos ferramentais, aqui estamos novamente no contexto em que saber o que fazer é a parte mais desafiadora. Ainda assim, construir features pode ter suas dificuldades, particularmente se estivermos tratando de dados menos usuais: como textos, datas e fatores. Seguem alguns exemplos destas classes de objetos, considerando os respectivos pacotes do tidyverse.


## textos (stringr)

Variáveis de texto são muito comuns nos bancos de dados e, geralmente, dão bastante trabalho para serem manipuladas.

Vantagens do stringr em relação às funções do R base:

Sintaxe unificada, o que auxilia na memorização das funções e leitura do código.
Todas as funções são vetorizadas.
Construído sobre a biblioteca ICU, implementada em C e C++, uma garantia de resultados mais rápidos.
Regras básicas do pacote:

As funções de manipulação de texto começam com str_. Caso esqueça o nome de uma função, basta digitar stringr::str_ e apertar TAB para ver quais são as opções.

O primeiro argumento da função é sempre uma string ou um vetor de strings.

Principais funções:

- str_detect() - identificar a presença de padrões em uma string;
- str_count() - contabiliza o número de vezes que um padrão é encontrado;
- str_replace() - substitui um dado padrão em uma string;
- str_to_lower() - converter strings maiúsculas e minúsculas

Exercício: Quantos `blue` existem na coluna skin_color?


```{r}
starwars %>% 
    #mutate(index_blue = ifelse(skin_color == "blue", "linhas com", "linhas sem")) %>% 
    mutate(index_blue = ifelse( str_detect(skin_color, "blue") ,
                                "linhas com", "linhas sem")) %>% 
    group_by(index_blue) %>% 
    count
```

Exercício: criar uma coluna com as skin_color unicas


```{r}
#base para exemplo
  fruit %>% 
    enframe %>% #versão tibble para vetores
    glimpse

#exemplo: obtenção da quantidade de palavras contendo a string "berry"
  fruit %>% 
    str_count("berry") %>% 
    sum()
    
```

## datas (lubridate)

Trabalhar com datas em qualquer linguagemm de programação costuma dar alguma dor de cabeça, muitas vezes com as lógicas sendo alteradas de acordo com o tipo do objeto que você está usando (data, hora, data/hora).Contudo, com o pacote {lubridate} passamos a ter uma sintaxe rica, contemplando :

    - `as_date()` - converter um objeto para uma data ou hora 
    - `wday()` - retorna o dia da semana como um número decimal 
    - `today()` - retorna a data corrente 
    - `floor_date()` - arredonda para o limite inferior mais próximo da unidade de tempo

```{r, error=TRUE, message=F}
#exemplo: retorna erro por não ser uma data válida
  birthday <- lubridate::dmy("29/02/1971")
  
#data ok
  birthday <- lubridate::dmy("29/02/1972"); birthday
  
#carregando `lubridate`, tornando o prefixo `lubridate::` desnecessário
  library(lubridate)
  
#obtenção do dia da semana da data especificada
  wday(birthday, label = TRUE)

```


As funções date() e as_date() assumem que a ordem segue o padrão da língua inglesa: ano-mês-dia (ymd). A função dmy() resolve esse problema estabelecendo explicitamente a ordem dia-mês-ano. Existem funções para todas as ordens possíveis: dmy(), mdy(), myd(), ymd(), ydm() etc.

Uma grande facilidade que essas funções trazem é poder criar objetos com classe date a partir de números e strings em diversos formatos.

```{r}
data_string <- "21-10-2015"
class(data_string)
## [1] "character"

data_date <- date(data_string)
class(data_date)
## [1] "Date"
data_date
## [1] "0021-10-20"

data_as_date <- as_date(data_string)
## Warning: All formats failed to parse. No formats found.
class(data_as_date)
## [1] "Date"
data_as_date
## [1] NA

data_mdy <- dmy(data_string)
class(data_mdy)
## [1] "Date"
data_mdy
## [1] "2015-10-21"
```

```{r}
#lubridate::

library(lubridate)

dmy(21102015)
## [1] "2015-10-21"
dmy("21102015")
## [1] "2015-10-21"
dmy("21/10/2015")
## [1] "2015-10-21"
dmy("21.10.2015")
## [1] "2015-10-21"

temp1 <- ymd_hms("2010-08-03 00:50:50") ; temp1
temp2 <- date(temp1) - today() ; temp2
temp3 <- temp1 + months(3) + weeks(1) ; temp3

rm(temp1, temp2, temp3)
```


## fatores (forcats)
Grande parte da frustração associada ao uso de fatores no R existe por conta da falta de algumas ferramentas úteis no pacote base. As principais funções do forcats servem para alterar a ordem e modificar os níveis de um fator.

- fct_count() - conta o número de valores de cada nível;
- fct_relevel() - reordenar os níveis dos fatores;
- fct_explicit_na() - adicionar o NA como um dos níveis


```{r}
#base para exemplo
  gss_cat %>% glimpse

#exemplo: contabilizando a quantidade de raças declaradas 
  gss_cat$race %>% 
  fct_count() 
```

```{r}
starwars %>% 
  mutate(gender = fct_explicit_na(gender)) %>% 
  group_by(gender) %>% 
  count
## # A tibble: 3 x 2
## # Groups:   gender [3]
##   gender        n
##   <fct>     <int>
## 1 feminine     17
## 2 masculine    66
## 3 (Missing)     4

starwars %>% 
    mutate(gender = fct_explicit_na(gender)) %>% 
    mutate(gender = fct_relevel(gender)) %>% 
    ggplot(aes(gender, height, color = gender)) + 
    geom_boxplot()
```



# Considerações Finais

Visto existirem muitos termos que são relacionados aos temas discutidos neste módulo, montei um pequeno resumo que espero que ajude.

- **Pré-processamento** trata de: representação, qualidade e limpeza. Tem como termos próximos, isto é, termos que possuem objetivos próximos, e/ou que irão retornar discussões importantes relacionadas ao tema: Data Wrangling, Data Munging, Data Cleaning, Cleansing  e Data Integrity.

- **Engenharia de Atributos**: Eloquência, Interpretabilidade, e Otimização de processamento. E tem como termos próximos, seguindo a mesma lógica do tópico anterior: Feature Engineering, Feature Extraction, Feature Selection e Data Transformation.


Sempre vale lembrar sobre como termos em inglês retornam mais resultados para estes temas, "engenharia de atributos" por exemplo soa até estranho para os ouvidos, pois o mais usual é a denominação "Feature Engineering".
 
 

# MÃO NA MASSA!

- o pacote `naniar` se mostrou uma ferramenta interessante no contexto de `missing data`, mas não exploramos todos os seus recursos. Por exemplo, a sequência de funções nos possibilita avaliar se existe uma mudança na distribuição dos dados faltantes:

```{r}
naniar::oceanbuoys %>%
    naniar::bind_shadow() %>%
    ggplot(aes(x = air_temp_c,
               fill = humidity_NA)) +
        geom_histogram()
```

Considerando o output gráfico, você diria que estamos lidando com qual tipo de missing aqui? Parece ser o caso de dados faltantes aleatórios? 


- O livro [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/) é uma das minhas referências preferidas! Não apenas pelos excelentes cases e discussões, mas por procurar desenvolver o(a) leitor(a) não apenas em termos técnicos, mas também com a intuição das técnicas. A tarefa aqui é: leia a seção de introdução de cada um dos capítulos para evoluir nos assuntos aqui abordados. Recomendo particularmente a leitura sobre encoding, do capítulo 5.


